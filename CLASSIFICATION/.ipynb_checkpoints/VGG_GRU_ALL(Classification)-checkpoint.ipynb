{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import numpy\n",
    "import dataset\n",
    "import dataset_test\n",
    "import random\n",
    "from utils import *\n",
    "from VGG_gru import FERANet\n",
    "# from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "import subprocess\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading vgg16 pre-fera2013 model...\n",
      "finish loading vgg16 pre-fera2013 model!\n"
     ]
    }
   ],
   "source": [
    "# model can be downloaded here: https://drive.google.com/drive/folders/1f17xgwvGaUpgXYBssocUNXDBgga-b3qp\n",
    "preInitial = True\n",
    "if preInitial == True:\n",
    "\tmodel = FERANet()\n",
    "\tmodel = Initial(model)\n",
    "else:\n",
    "\tmodel = FERANet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_batches = 0\n",
    "num_workers   = 0\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "random.seed(1)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "kwargs = {'num_workers': num_workers, 'pin_memory': True}\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 0.0001\n",
    "epochs = 20 \n",
    "length = 32\n",
    "\n",
    "use_Tensorboard = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO CHANGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TO CHANGE #######\n",
    "# EXP_TYPE = 'Regression/'\n",
    "EXP_TYPE = 'Classification/'\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9, weight_decay= 0.00005)\n",
    "\n",
    "#### Change loss function in UTILS.py\n",
    "\n",
    "metric = AccumulatedAccuracyMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO KEEP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,optimizer):\n",
    "      \n",
    "\ttrain_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset.listDataset(trainlist,length = length,\n",
    "\t\t\t\t\t   shuffle=True,\n",
    "\t\t\t\t\t   train=True,\n",
    "\t\t\t\t\t   dataset = dataset),\n",
    "\t\tbatch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\ttrain_learning_rate = float(param_group['lr'])\n",
    "\n",
    "\tlogging('epoch %d, processed %d samples, lr %f' % (epoch, epoch * len(train_loader.dataset), train_learning_rate))\n",
    "\n",
    "\trunning_loss = 0.0\n",
    "\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor batch_idx, (data, label) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "\t\tdata = data.squeeze(0)\n",
    "\t\tdata = Variable(data).cuda()\n",
    "\n",
    "\t\tlabel = Variable(label.long()).cuda()\n",
    "\t\tlabel = label.squeeze(1)\n",
    "        \n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\toutput = model(data)\n",
    "\n",
    "\t\tloss = loss_function(output, label)\n",
    "\n",
    "\t\trunning_loss += loss.data\n",
    "\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tif epoch %1 == 0:\n",
    "\t\tlogging('Train Loss:{:.6f}'.format(running_loss))\n",
    "        \n",
    "\treturn running_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(epoch,metric):\n",
    "\n",
    "\tmodel.eval()\n",
    "    \n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset.listDataset(vallist,length = length,       \n",
    "\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\ttrain=False,\n",
    "\t\t\t\t\tdataset = dataset),\n",
    "\t\t\t\t\tbatch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "\tfor batch_idx, (data, label) in enumerate(test_loader):\n",
    "\n",
    "\t\tdata = data.squeeze(0)\n",
    "\t\tBatch,T,C,H,W = data.size()\n",
    "\n",
    "\t\tdata = Variable(data,volatile=True).cuda()\n",
    "\n",
    "\t\tlabel = Variable(label.long(),volatile=True).cuda()\n",
    "\t\tlabel = label.squeeze(1)\n",
    "\t\toutput = []\n",
    "        \n",
    "\t\tfor batch_index in range(Batch):\n",
    "# \t\t\tprint(batch_index)            \n",
    "\t\t\toutput_feature = model(data[batch_index])\n",
    "\t\t\toutput.append(output_feature)\n",
    "\t\toutput = torch.mean(torch.cat(output), 0, keepdim=True) \n",
    "# \t\tprint(\"Dua's output is: \", output)\n",
    "\t\tmetric(output, label) \n",
    "\t\teval_loss = metric.value()\n",
    "\tif epoch %1 == 0:\n",
    "\t\tlogging(\"Eval Loss: %f\" % (eval_loss))\n",
    "    \n",
    "\treturn eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch,metric):\n",
    "    \n",
    "\tmodel.eval()\n",
    "\tresult = []    \n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset_test.listTESTDataset(testlist,length = length,\n",
    "# \t\tdataset.listDataset(vallist,length = length,        \n",
    "\t\t\t\t\tshuffle=False,\n",
    "# \t\t\t\t\ttrain=False,\n",
    "\t\t\t\t\tdataset = dataset),\n",
    "\t\t\t\t\tbatch_size=1, shuffle=False, **kwargs)\n",
    "    \n",
    "\tfor batch_idx, (data, label, participant) in enumerate(test_loader):\n",
    "\n",
    "\t\tdata = data.squeeze(0)\n",
    "\n",
    "\t\tBatch,T,C,H,W = data.size()\n",
    "\t\tdata = Variable(data,volatile=True).cuda()\n",
    "\n",
    "\t\tlabel = Variable(label.float(),volatile=True).cuda()\n",
    "# \t\tprint('Participant name:',participant)\n",
    "# \t\tprint('Participant label:',int(label))\n",
    "\t\toutput = []\n",
    "\t\tfor batch_index in range(Batch):           \n",
    "\t\t\toutput_feature = model(data[batch_index])\n",
    "\t\t\toutput.append(output_feature)\n",
    "\t\toutput = torch.mean(torch.cat(output), 0, keepdim=True)\n",
    "# \t\tprint(\"Participant output is: \", int(output))\n",
    "\t\tmetric(output, label) \n",
    "\t\ttest_loss = metric.value()\n",
    "# \t\tprint(\"Participant loss is: \", test_loss)\n",
    "\t\tresult.append([participant, int(label), int(output), test_loss])\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMB = ['BDI','AVEC','AVEC-BDI','AVEC-TEST','BDI-TEST']\n",
    "NUM = ['1','2','3','4','5']\n",
    "#Text file of image path for each of the above combinations\n",
    "textMAIN = '/timo/datasets/Dua/GRU-test/Data-Combinations/'\n",
    "textPATH = textMAIN+EXP_TYPE\n",
    "backupdir     = '/timo/datasets/Dua/GRU-test/Saved-Models'\n",
    "results_PATH = '/timo/datasets/Dua/GRU-test/Results/'\n",
    "#Datasets Path (participants Path) AVEC + BDI\n",
    "# path_d= '/home/duaa/Desktop/GRU-test/emotion_classification-master/VGG_GRU/dataset.py'\n",
    "#Where to save trained models Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isfile != True:\n",
    "# \tsubprocess.call([\"python\", path_d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training+Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-21 15:12:47 epoch 1, processed 36 samples, lr 0.000100\n",
      "2021-02-21 15:12:58 Train Loss:25.077845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duaa/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/duaa/anaconda3/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-21 15:15:45 Eval Loss: 7.233975\n",
      "2021-02-21 15:15:45 epoch 2, processed 72 samples, lr 0.000100\n",
      "2021-02-21 15:15:56 Train Loss:21.439127\n",
      "2021-02-21 15:18:41 Eval Loss: 13.735348\n",
      "2021-02-21 15:18:41 epoch 3, processed 108 samples, lr 0.000100\n",
      "2021-02-21 15:18:52 Train Loss:15.259271\n",
      "2021-02-21 15:21:38 Eval Loss: 19.153942\n",
      "2021-02-21 15:21:38 epoch 4, processed 144 samples, lr 0.000100\n",
      "2021-02-21 15:21:50 Train Loss:14.058403\n",
      "2021-02-21 15:24:34 Eval Loss: 25.148394\n",
      "2021-02-21 15:24:34 epoch 5, processed 180 samples, lr 0.000100\n",
      "2021-02-21 15:24:47 Train Loss:8.923861\n",
      "2021-02-21 15:27:32 Eval Loss: 34.116314\n",
      "2021-02-21 15:27:32 epoch 6, processed 216 samples, lr 0.000100\n",
      "2021-02-21 15:27:44 Train Loss:10.343703\n",
      "2021-02-21 15:30:28 Eval Loss: 40.467186\n",
      "2021-02-21 15:30:28 epoch 7, processed 252 samples, lr 0.000100\n",
      "2021-02-21 15:30:40 Train Loss:8.147131\n",
      "2021-02-21 15:33:24 Eval Loss: 48.066536\n",
      "2021-02-21 15:33:24 epoch 8, processed 288 samples, lr 0.000100\n",
      "2021-02-21 15:33:36 Train Loss:9.595623\n",
      "2021-02-21 15:36:21 Eval Loss: 55.903465\n",
      "2021-02-21 15:36:21 epoch 9, processed 324 samples, lr 0.000100\n",
      "2021-02-21 15:36:34 Train Loss:4.276244\n",
      "2021-02-21 15:39:19 Eval Loss: 66.552010\n",
      "2021-02-21 15:39:19 epoch 10, processed 360 samples, lr 0.000100\n",
      "2021-02-21 15:39:32 Train Loss:3.768125\n",
      "2021-02-21 15:42:16 Eval Loss: 75.243980\n",
      "2021-02-21 15:42:16 epoch 11, processed 396 samples, lr 0.000100\n",
      "2021-02-21 15:42:29 Train Loss:2.280047\n",
      "2021-02-21 15:45:14 Eval Loss: 82.548012\n",
      "2021-02-21 15:45:14 epoch 12, processed 432 samples, lr 0.000100\n",
      "2021-02-21 15:45:26 Train Loss:1.836511\n",
      "2021-02-21 15:48:10 Eval Loss: 90.107239\n",
      "2021-02-21 15:48:10 epoch 13, processed 468 samples, lr 0.000100\n",
      "2021-02-21 15:48:22 Train Loss:2.216860\n",
      "2021-02-21 15:51:06 Eval Loss: 97.517387\n",
      "2021-02-21 15:51:06 epoch 14, processed 504 samples, lr 0.000100\n",
      "2021-02-21 15:51:18 Train Loss:1.280574\n",
      "2021-02-21 15:54:05 Eval Loss: 107.319481\n",
      "2021-02-21 15:54:05 epoch 15, processed 540 samples, lr 0.000100\n",
      "2021-02-21 15:54:17 Train Loss:0.901133\n",
      "2021-02-21 15:57:03 Eval Loss: 115.146919\n",
      "2021-02-21 15:57:03 epoch 16, processed 576 samples, lr 0.000100\n",
      "2021-02-21 15:57:16 Train Loss:0.980670\n",
      "2021-02-21 16:00:01 Eval Loss: 125.792282\n",
      "2021-02-21 16:00:01 epoch 17, processed 612 samples, lr 0.000100\n",
      "2021-02-21 16:00:14 Train Loss:0.805947\n"
     ]
    }
   ],
   "source": [
    "for C in COMB:\n",
    "    for N in NUM:\n",
    "        train_val_loss = []\n",
    "        trainlist=textPATH+C+'train'+N+'.txt'\n",
    "        vallist=textPATH+C+'val'+N+'.txt'\n",
    "        for epoch in range(1, epochs+1): \n",
    "            train_loss = train(epoch,optimizer)\n",
    "            with torch.no_grad():\n",
    "                eval_loss = eval(epoch,metric)\n",
    "            train_val_loss.append([int(train_loss), int(eval_loss)]) \n",
    "        with open(results_PATH+C+'train_val'+N+'_loss.csv', 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerows(train_val_loss)\n",
    "        csvFile.close()\n",
    "        torch.save(model.state_dict(),'%s/model_%s%s.pkl' % (backupdir,C,N))\n",
    "        #add saving train +eval loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in COMB:\n",
    "    for N in NUM:\n",
    "        testlist=textPATH+C+'test'+N+'.txt'\n",
    "        model.load_state_dict(torch.load('%s/model_%s%s.pkl' % (backupdir,C,N)))\n",
    "        with torch.no_grad():\n",
    "            test_accuary = test(epoch,metric)\n",
    "        with open(results_PATH+C+'test'+N+'Results.csv', 'w') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerows(test_accuary)\n",
    "        csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/timo/datasets/Dua/GRU-test/Results/test.csv')\n",
    "# data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
